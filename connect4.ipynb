{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilise gym enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('gym_connect4:connect4-v0')\n",
    "\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network for action-value approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        return nn.Sequential(\n",
    "                nn.Linear(self.state_size, 24),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(24, 24),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(24, self.action_size))\n",
    "\n",
    "    def act(self, state):\n",
    "        # Explore\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(action_space_size)\n",
    "            \n",
    "        # Greedy\n",
    "        act_values = self.model.forward(state)\n",
    "        return np.argmax(act_values)\n",
    "    \n",
    "    def learn(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DQNAgent(42, 7)\n",
    "agent.act([0]*42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c6c6c14b1026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3a589117e3e8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.995\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3a589117e3e8>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Neural Net for Deep-Q learning Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_space_size, action_space_size)\n",
    "\n",
    "for episode in range(20000):\n",
    "    state = env.reset()\n",
    "    episode_rewards = 0\n",
    "    for t in range(100):\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-able with oponents move as the resulting state\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        episode_rewards += reward\n",
    "        \n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps. Player {} wins\".format(t+1, env.game.winner))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.653277939214514"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <itertools._grouper object at 0x7f30d470fa90>\n",
      "0 <itertools._grouper object at 0x7f30e46bd710>\n",
      "2 <itertools._grouper object at 0x7f30e46bdb90>\n",
      "4 <itertools._grouper object at 0x7f30d4705a10>\n",
      "2 <itertools._grouper object at 0x7f30d4705650>\n"
     ]
    }
   ],
   "source": [
    "x = [1, 1, 1, 0, 2, 4, 2, 2]\n",
    "from itertools import groupby\n",
    "for k, g in groupby(x):\n",
    "    print(k, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 6, 6, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {1:[1,2, 4], 2:[5,6 ,6], 3:[4]}\n",
    "[v for col in x.values() for v in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class connect4:\n",
    "    def __init__(self):\n",
    "        self.board = {1: [0]*6,\n",
    "                      2: [0]*6,\n",
    "                      3: [0]*6,\n",
    "                      4: [0]*6,\n",
    "                      5: [0]*6,\n",
    "                      6: [0]*6,\n",
    "                      7: [0]*6}\n",
    "        self.possible_moves = [i for i in range(1, 8)]\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "        \n",
    "    def make_move(self, move):\n",
    "        if move not in self.possible_moves:\n",
    "            raise ValueError(\"move is not possible\")\n",
    "            \n",
    "        # Switch the player and play their move in the top of the specified column\n",
    "        self.player = 1 if self.player != 1 else 2\n",
    "        top_of_col = self.board[move].index(0)\n",
    "        self.board[move][top_of_col] = self.player\n",
    "        \n",
    "        # Players have reached the top of column so remove it from possible actions\n",
    "        if top_of_col >= 5:\n",
    "            self.possible_moves.remove(move)\n",
    "            \n",
    "        if self.check_winner(move, top_of_col):\n",
    "            self.winner = self.player\n",
    "            \n",
    "    def check_winner(self, col, row):\n",
    "        # just to keep variable names short\n",
    "        val = self.player\n",
    "        brd = self.board\n",
    "\n",
    "        # check right \n",
    "        if col <= 4:\n",
    "            # Horizontal\n",
    "            if brd[col+1][row] == brd[col+2][row] == brd[col+3][row] == val:\n",
    "                return True\n",
    "            # Diagonal Down\n",
    "            if row >= 3 and brd[col+1][row-1] == brd[col+2][row-2] == brd[col+3][row-3] == val:\n",
    "                return True\n",
    "            # Diagonal Up\n",
    "            if row <= 2 and brd[col+1][row+1] == brd[col+2][row+2] == brd[col+3][row+3] == val:\n",
    "                return True\n",
    "        \n",
    "        # check left\n",
    "        if col >= 4:\n",
    "            # Horizontal\n",
    "            if brd[col-1][row] == brd[col-2][row] == brd[col-3][row] == val:\n",
    "                return True\n",
    "            # Diagonal Down\n",
    "            if row >= 3 and brd[col-1][row-1] == brd[col-2][row-2] == brd[col-3][row-3] == val:\n",
    "                return True\n",
    "            # Diagonal Up\n",
    "            if row <= 2 and brd[col-1][row+1] == brd[col-2][row+2] == brd[col-3][row+3] == val:\n",
    "                return True\n",
    "            \n",
    "        # check vertical\n",
    "        return any(1 for key, group in groupby(brd[col]) if len(list(group)) > 3 and key == val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "[0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 2]\n",
      "[0, 1, 1, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "game = connect4()\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "game.make_move(2)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(3)\n",
    "game.make_move(7)\n",
    "game.make_move(4)\n",
    "game.make_move(7)\n",
    "game.make_move(5)\n",
    "\n",
    "for x in range(5, -1, -1):\n",
    "    print([game.board[k][x] for k in game.board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\"\n",
    "    Board is indexed:\n",
    "        [0, 1, 2,\n",
    "         3, 4, 5,\n",
    "         6, 7, 8]\n",
    "         \n",
    "         and contains either 0 or the player number (1 or 2)\n",
    "         \n",
    "    player represents the player who last moved\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = [0 for i in range(9)]\n",
    "        self.possible_moves = [i for i in range(9)]\n",
    "        self.player = 0\n",
    "    \n",
    "    def make_move(self, move):\n",
    "        assert move in self.possible_moves\n",
    "        \n",
    "        self.player = 1 if self.player != 1 else 2\n",
    "        self.board[move] = self.player\n",
    "        self.possible_moves.remove(move)\n",
    "        \n",
    "    def winner(self):\n",
    "        if all(self.board):\n",
    "            return 0\n",
    "        \n",
    "        lines = ((0,1,2),\n",
    "                 (3,4,5),\n",
    "                 (6,7,8),\n",
    "                 (0,3,6),\n",
    "                 (1,4,7),\n",
    "                 (2,5,8),\n",
    "                 (0,4,8),\n",
    "                 (2,4,6))\n",
    "        \n",
    "        for i, j, k in lines:\n",
    "            player = self.board[i]\n",
    "            if player and player == self.board[j] == self.board[k]:\n",
    "                return player\n",
    "            \n",
    "        return -1\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.board[:3])\n",
    "        print(self.board[3:6])\n",
    "        print(self.board[6:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "Enter a move: 3\n",
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 0, 0]\n",
      "Enter a move: 3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-23fe6536069c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a move: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nand the winner is: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-1eb161ecf25c>\u001b[0m in \u001b[0;36mmake_move\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         if move not in self.possible_moves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             raise ValueError(\"move is not possible\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while x.winner() is -1:\n",
    "    x.render()\n",
    "    x.make_move(int(input(\"Enter a move: \")))\n",
    "    \n",
    "print(\"\\n\\nand the winner is: {}\".format(x.winner()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
