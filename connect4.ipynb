{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilise gym enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 42)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make('gym_connect4:connect4-v0')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "num_actions, state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1.0   # exploration rate\n",
    "epsilon_min = 0.001\n",
    "epsilon_decay = 0.9995\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network\n",
    "\n",
    "    Input:  state observation\n",
    "    \n",
    "    Output: policy distribution\n",
    "            value prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "    \n",
    "model = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition', 'state action n_state reward done')\n",
    "\n",
    "# model = DQN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "dev = 0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1041, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.FloatTensor(state).cuda()\n",
    "model(state).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 750\n",
    "num_episodes = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decay():\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "        \n",
    "    return epsilon\n",
    "        \n",
    "plt.plot([decay() for t in range(9000)])\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state):\n",
    "    # Decay exploration rate\n",
    "    decay()\n",
    "    \n",
    "    # Convert state represenation into tensor\n",
    "    state = torch.tensor(state, dtype=torch.float, device=dev)\n",
    "    \n",
    "    # 𝜺-greedy exploration\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    \n",
    "    return model(state).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch):\n",
    "    states = torch.tensor([x.state for x in batch], dtype=torch.float, device=dev)\n",
    "    actions = torch.as_tensor([x.action for x in batch], dtype=torch.long, device=dev)\n",
    "    rewards = torch.as_tensor([x.reward for x in batch], dtype=torch.float, device=dev)\n",
    "    next_states = torch.as_tensor([x.n_state for x in batch], dtype=torch.float, device=dev)\n",
    "    done = torch.as_tensor([x.done for x in batch], device=dev)\n",
    "\n",
    "    q_values      = model(states)\n",
    "    next_q_values = model(next_states)\n",
    "\n",
    "    q_value          = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = rewards + gamma * next_q_value * ~done\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(q_value, expected_q_value)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Connect4' object has no attribute 'possible_moves'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-5676f375de3d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;31m# Choose an action\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mmemory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTransition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/gym-tictactoe/gym_connect4/envs/connect4_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0maction\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpossible_moves\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m       \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Connect4' object has no attribute 'possible_moves'"
     ]
    }
   ],
   "source": [
    "memory = []\n",
    "memory.clear()\n",
    "loss = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(10000):\n",
    "        # Choose an action\n",
    "        action = act(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.append(Transition(state, action, new_state, reward, done))\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if len(memory) > batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            l = train(batch)\n",
    "            loss.append(l)\n",
    "            print(\"training batch... got loss \", l)\n",
    "            memory.clear()\n",
    "            \n",
    "        if done:\n",
    "#             print(\"Episode finished after {} timesteps. Player {} wins\".format(t+1, env.game.winner))\n",
    "#             env.game.show()\n",
    "            break\n",
    "    \n",
    "env.close()\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Connect4' object has no attribute 'possible_moves'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-3f48e8ac8683>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/gym-tictactoe/gym_connect4/envs/connect4_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0maction\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpossible_moves\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m       \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Connect4' object has no attribute 'possible_moves'"
     ]
    }
   ],
   "source": [
    "# Take only greedy actions\n",
    "epsilon = 0\n",
    "\n",
    "start, done = env.reset(), False\n",
    "\n",
    "while not done:\n",
    "    action = act(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.game.show()\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.99995\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        return nn.Sequential(\n",
    "                nn.Linear(self.state_size, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 52),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(52, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, self.action_size))\n",
    "\n",
    "    def act(self, state):\n",
    "        self.decay()\n",
    "        # Explore\n",
    "        print(self.epsilon)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(action_space_size)\n",
    "            \n",
    "        # Greedy\n",
    "        state = torch.FloatTensor(state)\n",
    "        act_values = self.model(state)\n",
    "        return act_values.argmax().item()\n",
    "    \n",
    "    def decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train(self, state, new_state, action, reward):\n",
    "        state = torch.FloatTensor(state)\n",
    "        reward = torch.tensor([])\n",
    "        print(reward, type(reward))\n",
    "        \n",
    "        q_value = self.model(state)[action]\n",
    "        \n",
    "        loss = self.loss(q_value, reward)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        print(loss.item(), q_value, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <itertools._grouper object at 0x7f30d470fa90>\n",
      "0 <itertools._grouper object at 0x7f30e46bd710>\n",
      "2 <itertools._grouper object at 0x7f30e46bdb90>\n",
      "4 <itertools._grouper object at 0x7f30d4705a10>\n",
      "2 <itertools._grouper object at 0x7f30d4705650>\n"
     ]
    }
   ],
   "source": [
    "x = [1, 1, 1, 0, 2, 4, 2, 2]\n",
    "from itertools import groupby\n",
    "for k, g in groupby(x):\n",
    "    print(k, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 6, 6, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {1:[1,2, 4], 2:[5,6 ,6], 3:[4]}\n",
    "[v for col in x.values() for v in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class connect4:\n",
    "    def __init__(self):\n",
    "        self.board = {1: [0]*6,\n",
    "                      2: [0]*6,\n",
    "                      3: [0]*6,\n",
    "                      4: [0]*6,\n",
    "                      5: [0]*6,\n",
    "                      6: [0]*6,\n",
    "                      7: [0]*6}\n",
    "        self.possible_moves = [i for i in range(1, 8)]\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "        \n",
    "    def make_move(self, move):\n",
    "        if move not in self.possible_moves:\n",
    "            raise ValueError(\"move is not possible\")\n",
    "            \n",
    "        # Switch the player and play their move in the top of the specified column\n",
    "        self.player = 1 if self.player != 1 else 2\n",
    "        top_of_col = self.board[move].index(0)\n",
    "        self.board[move][top_of_col] = self.player\n",
    "        \n",
    "        # Players have reached the top of column so remove it from possible actions\n",
    "        if top_of_col >= 5:\n",
    "            self.possible_moves.remove(move)\n",
    "            \n",
    "        if self.check_winner(move, top_of_col):\n",
    "            self.winner = self.player\n",
    "            \n",
    "    def check_winner(self, col, row):\n",
    "        # just to keep variable names short\n",
    "        val = self.player\n",
    "        brd = self.board\n",
    "\n",
    "        # check right \n",
    "        if col <= 4:\n",
    "            # Horizontal\n",
    "            if brd[col+1][row] == brd[col+2][row] == brd[col+3][row] == val:\n",
    "                return True\n",
    "            # Diagonal Down\n",
    "            if row >= 3 and brd[col+1][row-1] == brd[col+2][row-2] == brd[col+3][row-3] == val:\n",
    "                return True\n",
    "            # Diagonal Up\n",
    "            if row <= 2 and brd[col+1][row+1] == brd[col+2][row+2] == brd[col+3][row+3] == val:\n",
    "                return True\n",
    "        \n",
    "        # check left\n",
    "        if col >= 4:\n",
    "            # Horizontal\n",
    "            if brd[col-1][row] == brd[col-2][row] == brd[col-3][row] == val:\n",
    "                return True\n",
    "            # Diagonal Down\n",
    "            if row >= 3 and brd[col-1][row-1] == brd[col-2][row-2] == brd[col-3][row-3] == val:\n",
    "                return True\n",
    "            # Diagonal Up\n",
    "            if row <= 2 and brd[col-1][row+1] == brd[col-2][row+2] == brd[col-3][row+3] == val:\n",
    "                return True\n",
    "            \n",
    "        # check vertical\n",
    "        return any(1 for key, group in groupby(brd[col]) if len(list(group)) > 3 and key == val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "[0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 2]\n",
      "[0, 1, 1, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "game = connect4()\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(2)\n",
    "game.make_move(2)\n",
    "game.make_move(2)\n",
    "print(game.winner)\n",
    "game.make_move(3)\n",
    "game.make_move(7)\n",
    "game.make_move(4)\n",
    "game.make_move(7)\n",
    "game.make_move(5)\n",
    "\n",
    "for x in range(5, -1, -1):\n",
    "    print([game.board[k][x] for k in game.board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\"\n",
    "    Board is indexed:\n",
    "        [0, 1, 2,\n",
    "         3, 4, 5,\n",
    "         6, 7, 8]\n",
    "         \n",
    "         and contains either 0 or the player number (1 or 2)\n",
    "         \n",
    "    player represents the player who last moved\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = [0 for i in range(9)]\n",
    "        self.possible_moves = [i for i in range(9)]\n",
    "        self.player = 0\n",
    "    \n",
    "    def make_move(self, move):\n",
    "        assert move in self.possible_moves\n",
    "        \n",
    "        self.player = 1 if self.player != 1 else 2\n",
    "        self.board[move] = self.player\n",
    "        self.possible_moves.remove(move)\n",
    "        \n",
    "    def winner(self):\n",
    "        if all(self.board):\n",
    "            return 0\n",
    "        \n",
    "        lines = ((0,1,2),\n",
    "                 (3,4,5),\n",
    "                 (6,7,8),\n",
    "                 (0,3,6),\n",
    "                 (1,4,7),\n",
    "                 (2,5,8),\n",
    "                 (0,4,8),\n",
    "                 (2,4,6))\n",
    "        \n",
    "        for i, j, k in lines:\n",
    "            player = self.board[i]\n",
    "            if player and player == self.board[j] == self.board[k]:\n",
    "                return player\n",
    "            \n",
    "        return -1\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.board[:3])\n",
    "        print(self.board[3:6])\n",
    "        print(self.board[6:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-24f5383632d9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwhile\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwinner\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_move\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Enter a move: \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n\\nand the winner is: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwinner\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "while x.winner() is -1:\n",
    "    x.render()\n",
    "    x.make_move(int(input(\"Enter a move: \")))\n",
    "    \n",
    "print(\"\\n\\nand the winner is: {}\".format(x.winner()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}